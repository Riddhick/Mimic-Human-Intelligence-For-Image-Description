{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8kuhc8_s4kC",
        "outputId": "7f7b4356-3d15-4a6b-99de-943d74b2e9aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\EXTREME_RED\\AppData\\Local\\Temp\\ipykernel_26956\\3236133411.py:6: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#imports\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.init as init\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "import torchvision.transforms.functional as f\n",
        "import torchvision.models as models\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from dgl.nn import GraphConv\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyABT53ws4kH"
      },
      "outputs": [],
      "source": [
        "#location of the data\n",
        "data_location =  \"Flicker8k_Dataset/\"\n",
        "!ls $data_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v_rOJTLs4kL"
      },
      "outputs": [],
      "source": [
        "#reading the text data\n",
        "caption_file = 'captions.txt'\n",
        "df = pd.read_csv(caption_file)\n",
        "print(\"There are {} image to captions\".format(len(df)))\n",
        "df.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ2hbm6Ks4kO"
      },
      "outputs": [],
      "source": [
        "#using spacy for the better text tokenization\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S_EvzXPs4kQ"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self,freq_threshold):\n",
        "\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}\n",
        "\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self,text):\n",
        "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtvdkcx8s4kS"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectorAndFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True, iou_threshold=0.4):\n",
        "        super(ObjectDetectorAndFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Object detection model (Faster R-CNN)\n",
        "        self.object_detection_model = fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
        "        self.object_detection_model.eval()\n",
        "\n",
        "        # Feature extraction model (VGG19)\n",
        "        vgg = models.vgg19(pretrained=pretrained)\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.feature_extractor = vgg.features[:36]\n",
        "        self.feature_extractor = nn.Sequential(*self.feature_extractor)\n",
        "        self.iou_threshold = iou_threshold\n",
        "\n",
        "    def forward(self, image):\n",
        "        #graph_list = []\n",
        "        image = image.to(device)\n",
        "        #for image in images:\n",
        "        # Perform object detection\n",
        "        with torch.no_grad():\n",
        "            detections = self.object_detection_model([image])\n",
        "\n",
        "        g = dgl.DGLGraph()\n",
        "        g = g.to(device)\n",
        "\n",
        "        if len(detections[0]['boxes']) == 0:\n",
        "            # No objects detected; use whole image features\n",
        "            image_feature = self.feature_extractor(image.unsqueeze(0))\n",
        "            image_feature = image_feature.view(image_feature.size(0), 49, -1)  # (batch_size, 49, 2048)\n",
        "            g.add_nodes(1, {'features': image_feature})\n",
        "            g.add_edges(0, 0)  # self-loop edge\n",
        "            #graph_list.append(g)\n",
        "            return g\n",
        "\n",
        "        # Add nodes for each detected object and assign features\n",
        "        for box in detections[0]['boxes']:\n",
        "            x1, y1, x2, y2 = map(int, box.tolist())\n",
        "            if x1 >= x2 or y1 >= y2:\n",
        "                # Invalid bounding box, skip it\n",
        "                continue\n",
        "\n",
        "            # Extract object region\n",
        "            object_region = image[:, y1:y2, x1:x2]\n",
        "            if object_region.size(1) == 0 or object_region.size(2) == 0:\n",
        "                # Zero-size crop, skip it\n",
        "                continue\n",
        "\n",
        "            # Resize to a common size\n",
        "            object_region = f.resize(object_region, (224, 224))\n",
        "\n",
        "            # Extract features using VGG19\n",
        "            features = self.feature_extractor(object_region.unsqueeze(0))\n",
        "            features = features.view(features.size(0), 49, -1)  # (batch_size, 49, 2048)\n",
        "            g.add_nodes(1, {'features': features})\n",
        "\n",
        "        num_objects = g.number_of_nodes()\n",
        "\n",
        "        # Add edges based on IoU\n",
        "        for i in range(num_objects):\n",
        "            for j in range(num_objects):\n",
        "                if i != j:\n",
        "                    bbox_i = detections[0]['boxes'][i]\n",
        "                    bbox_j = detections[0]['boxes'][j]\n",
        "                    iou = self.calculate_iou(bbox_i, bbox_j)\n",
        "                    if iou > self.iou_threshold:\n",
        "                        g.add_edges(i, j)\n",
        "\n",
        "        #graph_list.append(g)\n",
        "        #g = dgl.batch(graph_list)\n",
        "        return g\n",
        "\n",
        "    def calculate_iou(self, bbox1, bbox2):\n",
        "        xmin = max(bbox1[0], bbox2[0])\n",
        "        ymin = max(bbox1[1], bbox2[1])\n",
        "        xmax = min(bbox1[2], bbox2[2])\n",
        "        ymax = min(bbox1[3], bbox2[3])\n",
        "\n",
        "        intersection_area = max(0, xmax - xmin) * max(0, ymax - ymin)\n",
        "\n",
        "        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
        "        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
        "        union_area = area1 + area2 - intersection_area\n",
        "\n",
        "        if union_area == 0:\n",
        "            return 0\n",
        "\n",
        "        iou = intersection_area / union_area\n",
        "        return iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j9eQc1ks4kU"
      },
      "outputs": [],
      "source": [
        "class FlickrDataset(Dataset):\n",
        "    \"\"\"\n",
        "    FlickrDataset\n",
        "    \"\"\"\n",
        "    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        #Get image and caption colum from the dataframe\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        #Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "\n",
        "        self.graphs = ObjectDetectorAndFeatureExtractor().to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        caption = self.captions[idx]\n",
        "        img_name = self.imgs[idx]\n",
        "        img_location = os.path.join(self.root_dir,img_name)\n",
        "        img = Image.open(img_location).convert(\"RGB\")\n",
        "\n",
        "        #apply the transfromation to the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        #numericalize the caption text\n",
        "        caption_vec = []\n",
        "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
        "        caption_vec += self.vocab.numericalize(caption)\n",
        "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        # Convert PIL Image to Tensor\n",
        "        #img_tensor = T.ToTensor()(img).to(self.device)\n",
        "\n",
        "        # Get the object detection graph\n",
        "        graph = self.graphs(img)\n",
        "\n",
        "        return img, torch.tensor(caption_vec), graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dxA70sUs4kX"
      },
      "outputs": [],
      "source": [
        "def show_image(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05Mg2BOHs4kZ"
      },
      "outputs": [],
      "source": [
        "class CapsCollate:\n",
        "\n",
        "    def __init__(self, pad_idx, batch_first=False):\n",
        "        self.pad_idx = pad_idx\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # Extract images, captions, and graphs from the batch\n",
        "        imgs = [item[0] for item in batch]\n",
        "        captions = [item[1] for item in batch]\n",
        "        graphs = [item[2] for item in batch]\n",
        "\n",
        "        # Concatenate images into a single tensor\n",
        "        imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "        # Pad captions to ensure consistent lengths within the batch\n",
        "        padded_captions = pad_sequence(captions, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
        "\n",
        "        # Batch the graphs to handle multiple graphs in a single input\n",
        "        batched_graphs = dgl.batch(graphs)\n",
        "\n",
        "        return imgs, padded_captions, batched_graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "3khnskAis4kb"
      },
      "outputs": [],
      "source": [
        "#Helper function to plot the Tensor image\n",
        "def show_image(img, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    #unnormalize\n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224\n",
        "    img[2] = img[2] * 0.225\n",
        "    img[0] += 0.485\n",
        "    img[1] += 0.456\n",
        "    img[2] += 0.406\n",
        "\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "\n",
        "\n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55VTTlius4kd"
      },
      "outputs": [],
      "source": [
        "class test_FlickrDataset(Dataset):\n",
        "    \"\"\"\n",
        "    FlickrDataset\n",
        "    \"\"\"\n",
        "    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        #Get image and caption colum from the dataframe\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        #Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "\n",
        "        self.graphs = ObjectDetectorAndFeatureExtractor().to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        caption = self.captions[idx]\n",
        "        img_name = self.imgs[idx]\n",
        "        img_location = os.path.join(self.root_dir,img_name)\n",
        "        img = Image.open(img_location).convert(\"RGB\")\n",
        "\n",
        "        #apply the transfromation to the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        #numericalize the caption text\n",
        "        caption_vec = []\n",
        "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
        "        caption_vec += self.vocab.numericalize(caption)\n",
        "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        # Convert PIL Image to Tensor\n",
        "        #img_tensor = T.ToTensor()(img).to(self.device)\n",
        "\n",
        "        # Get the object detection graph\n",
        "        graph = self.graphs(img)\n",
        "\n",
        "        return img, torch.tensor(caption_vec), graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "9KLS7DYIs4kd",
        "outputId": "35ef45db-5845-4910-f8b3-588fe25d2ceb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "#Initiate the Dataset and Dataloader\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "#defining the transform to be applied\n",
        "transforms = T.Compose([\n",
        "    T.Resize(226),\n",
        "    T.RandomCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "\n",
        "#dataset class\n",
        "train_dataset =  FlickrDataset(\n",
        "    root_dir = data_location,\n",
        "    captions_file = \"captions.txt\",\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "test_dataset =  FlickrDataset(\n",
        "    root_dir = data_location,\n",
        "    captions_file = \"TestImages.txt\",\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "#token to represent the padding\n",
        "pad_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
        "    # batch_first=False\n",
        ")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
        "    # batch_first=False\n",
        ")\n",
        "#vocab_size\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "dB1Zq7x-s4kf"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "\n",
        "        # Load pre-trained VGG19 for feature extraction\n",
        "        vgg = models.vgg19(pretrained=pretrained)\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        self.vgg_features = vgg.features[:36]\n",
        "        self.vgg_features = nn.Sequential(*self.vgg_features)\n",
        "\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.vgg_features(images)  # (batch_size, 512, 14, 14)\n",
        "\n",
        "        # Reshape the feature map\n",
        "        features = features.view(features.size(0), -1, 49)  # (batch_size, 2048, 49)\n",
        "        features = features.permute(0, 2, 1)  # (batch_size, 49, 2048)\n",
        "\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYVM3FC4s4kg"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # Define GCN layers\n",
        "        self.conv1 = GraphConv(in_feats, in_feats,allow_zero_in_degree=True)\n",
        "        self.conv2 = GraphConv(in_feats, in_feats,allow_zero_in_degree=True)\n",
        "\n",
        "        # Define linear layer for edge computation\n",
        "        self.edge_linear = nn.Linear(in_feats * 2, in_feats)\n",
        "\n",
        "    def forward(self, g):\n",
        "\n",
        "        h = g.ndata['features']\n",
        "        # Perform GCN\n",
        "        h = self.conv1(g, h)\n",
        "        h = torch.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        h = torch.relu(h)\n",
        "        # Concatenate src and dest node features for all edges\n",
        "        src_feats = h[g.edges()[0]]\n",
        "        dst_feats = h[g.edges()[1]]\n",
        "\n",
        "        edge_feats = torch.cat([src_feats, dst_feats], dim=2)\n",
        "        edge_values = self.edge_linear(edge_feats).squeeze(dim=1)\n",
        "\n",
        "        g.edata['edge_values'] = edge_values\n",
        "\n",
        "        mean_edge_feat = dgl.readout_edges(g, 'edge_values', op='mean')\n",
        "\n",
        "        return mean_edge_feat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr-AYDWCs4ki"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, attention_dim):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_dim = attention_dim\n",
        "        self.head_dim = attention_dim // num_heads\n",
        "\n",
        "        self.linear = nn.Linear(input_dim, attention_dim)\n",
        "        self.output_linear = nn.Linear(attention_dim, input_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Use Xavier/Glorot initialization for the linear layers\n",
        "        init.xavier_uniform_(self.linear.weight)\n",
        "        init.xavier_uniform_(self.output_linear.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size, seq_length, input_dim = inputs.size()\n",
        "\n",
        "        projected_inputs = self.linear(inputs)  # (batch_size, seq_length, attention_dim)\n",
        "        reshaped_inputs = projected_inputs.view(batch_size, seq_length, self.num_heads, self.head_dim) \\\n",
        "                                          .permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        # Calculate attention energy\n",
        "        transposed_inputs = reshaped_inputs.transpose(-2, -1)  # (batch_size, num_heads, head_dim, seq_length)\n",
        "        energy = torch.matmul(reshaped_inputs, transposed_inputs)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention = F.softmax(energy / torch.sqrt(torch.tensor(self.head_dim, dtype=inputs.dtype)), dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        # Apply attention to get the context\n",
        "        context = torch.matmul(attention, reshaped_inputs)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        # Rearrange context and prepare output\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.attention_dim)  # (batch_size, seq_length, attention_dim)\n",
        "        output = self.output_linear(context)  # (batch_size, seq_length, input_dim)\n",
        "        attention = attention.permute(0, 2, 1, 3)  # (batch_size, seq_length, num_heads , seq_length)\n",
        "\n",
        "        # Optionally return attention for visualization/debugging\n",
        "        return output, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDOga-6Ss4kj"
      },
      "outputs": [],
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, num_heads, attention_dim):\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_dim = attention_dim\n",
        "        self.head_dim = attention_dim // num_heads\n",
        "\n",
        "        self.encoder_linear = nn.Linear(encoder_dim, attention_dim)\n",
        "        self.decoder_linear = nn.Linear(decoder_dim, self.head_dim)\n",
        "        self.output_linear = nn.Linear(attention_dim, decoder_dim)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize weights with Xavier/Glorot\n",
        "        init.xavier_uniform_(self.encoder_linear.weight)\n",
        "        init.xavier_uniform_(self.decoder_linear.weight)\n",
        "        init.xavier_uniform_(self.output_linear.weight)\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        batch_size, num_features, _ = encoder_out.size()\n",
        "\n",
        "        encoder_proj = self.encoder_linear(encoder_out)  # (batch_size, num_features, attention_dim)\n",
        "        decoder_proj = self.decoder_linear(decoder_hidden)  # (batch_size, head_dim)\n",
        "\n",
        "        # Prepare the reshaped encoder projection\n",
        "        encoder_proj = encoder_proj.view(batch_size, self.num_heads, num_features, self.head_dim)  # (batch_size, num_heads, num_features, head_dim)\n",
        "\n",
        "        # Reshape decoder projection for cross-attention\n",
        "        decoder_proj = decoder_proj.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, head_dim)\n",
        "\n",
        "        # Compute energy for cross-attention\n",
        "        energy = torch.matmul(encoder_proj, decoder_proj.transpose(-2, -1))  # (batch_size, num_heads, num_features, 1)\n",
        "\n",
        "        # Apply scaled dot-product softmax for attention scores\n",
        "        attention = F.softmax(energy / torch.sqrt(torch.tensor(self.head_dim, dtype=encoder_out.dtype)), dim=-2)  # (batch_size, num_heads, num_features, 1)\n",
        "\n",
        "        # Get context by applying attention to encoder projections\n",
        "        context = torch.matmul(attention.transpose(-2, -1), encoder_proj).squeeze(-2)  # (batch_size, num_heads, head_dim)\n",
        "\n",
        "        # Merge multi-head context into one attention_dim\n",
        "        context = context.view(batch_size, self.head_dim * self.num_heads)  # (batch_size, attention_dim)\n",
        "\n",
        "        # Final output with a linear layer\n",
        "        output = self.output_linear(context)  # (batch_size, decoder_dim)\n",
        "\n",
        "        return output, attention.squeeze(-1)  # Squeeze to get (batch_size, num_heads, num_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c33MuTUWs4kk"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, attention_dim, gcn_encode_dim, encoder_dim, decoder_dim, num_heads, drop_prob=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Save the model parameters\n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_dim = attention_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.edge_feature = GCN(gcn_encode_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # Multi-head cross attention\n",
        "        self.attention = MultiHeadCrossAttention(encoder_dim, decoder_dim, num_heads, attention_dim)\n",
        "        # Multi-head self-attention\n",
        "        self.attention_self = MultiHeadSelfAttention(embed_size, num_heads, attention_dim)\n",
        "\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.lstm_cell = nn.LSTMCell(embed_size + decoder_dim, decoder_dim, bias=True)\n",
        "        self.encoded = nn.Dropout(drop_prob)\n",
        "        self.fcn = nn.Linear(decoder_dim, vocab_size)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "\n",
        "    def forward(self, encoded_input, captions, graphs):\n",
        "        embeds = self.embedding(captions)\n",
        "\n",
        "        seq_length = len(captions[0])-1 # Exclude the last one\n",
        "        batch_size = captions.size(0)\n",
        "\n",
        "        edge_features = self.edge_feature(graphs)\n",
        "        #encoder_out = encoded_input + edge_features\n",
        "        encoder_out = torch.cat((encoded_input, edge_features), dim=2)\n",
        "        encoder_out = self.encoded(encoder_out)\n",
        "\n",
        "        h, c = self.init_hidden_state(encoder_out)\n",
        "\n",
        "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(encoder_out.device)\n",
        "        alphas = torch.zeros(batch_size, seq_length, self.num_heads,  encoder_out.size(1)).to(encoder_out.device)\n",
        "        alphas_self = torch.zeros(batch_size,  seq_length+1, self.num_heads,  (seq_length+1)).to(encoder_out.device)\n",
        "\n",
        "        for s in range(seq_length):\n",
        "            # Multi-head self-attention over captions\n",
        "            embeds_attended_self, alpha_self = self.attention_self(embeds)\n",
        "\n",
        "            # Multi-head cross-attention with encoder output\n",
        "            output, alpha = self.attention(encoder_out, h)\n",
        "\n",
        "            lstm_input = torch.cat((embeds_attended_self[:, s], output), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "\n",
        "            preds[:, s] = output\n",
        "            alphas[:, s] = alpha\n",
        "            alphas_self[:,s] = alpha_self[:,-1,:,:]\n",
        "\n",
        "        return preds, alphas, alphas_self\n",
        "\n",
        "    def generate_caption(self, encoder_out, max_len=25, vocab=None):\n",
        "        batch_size = encoder_out.size(0)\n",
        "        h, c = self.init_hidden_state(encoder_out)\n",
        "\n",
        "        alphas = []\n",
        "        alphas_self = []\n",
        "        captions = []\n",
        "\n",
        "        word = torch.tensor(vocab.stoi['<SOS>']).view(1, -1).to(encoder_out.device)\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "        for i in range(max_len):\n",
        "            embeds_attended_self, alpha_self = self.attention_self(embeds)\n",
        "            alphas_self.append(alpha_self.cpu().detach().numpy())\n",
        "\n",
        "            output, alpha = self.attention(encoder_out, h)\n",
        "            alphas.append(alpha.cpu().detach().numpy())\n",
        "\n",
        "            lstm_input = torch.cat((embeds_attended_self[:, 0], output), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "            output = output.view(batch_size, -1)\n",
        "\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            captions.append(predicted_word_idx.item())\n",
        "\n",
        "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "\n",
        "        return [vocab.itos[idx] for idx in captions], alphas, alphas_self\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "aQVN53IPs4kl"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,gcn_encode_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN()\n",
        "        #self.obj_feature = ObjectDetectorAndFeatureExtractor()\n",
        "        #self.edge_feature = GCN(2048, 512, 2048)\n",
        "        self.decoder = Decoder(\n",
        "            embed_size=embed_size,\n",
        "            vocab_size = len(train_dataset.vocab),\n",
        "            attention_dim=attention_dim,\n",
        "            gcn_encode_dim = gcn_encode_dim,\n",
        "            encoder_dim=encoder_dim,\n",
        "            decoder_dim=decoder_dim,\n",
        "            num_heads = 2\n",
        "        )\n",
        "\n",
        "    def forward(self, images, captions, graphs):\n",
        "        features = self.encoder(images)\n",
        "\n",
        "        outputs = self.decoder(features, captions, graphs)\n",
        "\n",
        "        #print(graphs[0])\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "__bL-MA4s4km",
        "outputId": "ad7ec9f1-f93c-45f9-8563-ecf01aac4b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EncoderDecoder(\n",
            "  (encoder): EncoderCNN(\n",
            "    (vgg_features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): ReLU(inplace=True)\n",
            "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (15): ReLU(inplace=True)\n",
            "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (17): ReLU(inplace=True)\n",
            "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (20): ReLU(inplace=True)\n",
            "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (22): ReLU(inplace=True)\n",
            "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (24): ReLU(inplace=True)\n",
            "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (26): ReLU(inplace=True)\n",
            "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (29): ReLU(inplace=True)\n",
            "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (31): ReLU(inplace=True)\n",
            "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (33): ReLU(inplace=True)\n",
            "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (35): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (edge_feature): GCN(\n",
            "      (conv1): GraphConv(in=2048, out=2048, normalization=both, activation=None)\n",
            "      (conv2): GraphConv(in=2048, out=2048, normalization=both, activation=None)\n",
            "      (edge_linear): Linear(in_features=4096, out_features=2048, bias=True)\n",
            "    )\n",
            "    (embedding): Embedding(2994, 200)\n",
            "    (attention): MultiHeadCrossAttention(\n",
            "      (encoder_linear): Linear(in_features=4096, out_features=256, bias=True)\n",
            "      (decoder_linear): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (attention_self): MultiHeadSelfAttention(\n",
            "      (linear): Linear(in_features=200, out_features=256, bias=True)\n",
            "      (output_linear): Linear(in_features=256, out_features=200, bias=True)\n",
            "    )\n",
            "    (init_h): Linear(in_features=4096, out_features=256, bias=True)\n",
            "    (init_c): Linear(in_features=4096, out_features=256, bias=True)\n",
            "    (lstm_cell): LSTMCell(456, 256)\n",
            "    (encoded): Dropout(p=0.2, inplace=False)\n",
            "    (fcn): Linear(in_features=256, out_features=2994, bias=True)\n",
            "    (drop): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            ")\n",
            "Total parameters: 42255178\n",
            "encoder.vgg_features.0.weight torch.Size([64, 3, 3, 3])\n",
            "encoder.vgg_features.0.bias torch.Size([64])\n",
            "encoder.vgg_features.2.weight torch.Size([64, 64, 3, 3])\n",
            "encoder.vgg_features.2.bias torch.Size([64])\n",
            "encoder.vgg_features.5.weight torch.Size([128, 64, 3, 3])\n",
            "encoder.vgg_features.5.bias torch.Size([128])\n",
            "encoder.vgg_features.7.weight torch.Size([128, 128, 3, 3])\n",
            "encoder.vgg_features.7.bias torch.Size([128])\n",
            "encoder.vgg_features.10.weight torch.Size([256, 128, 3, 3])\n",
            "encoder.vgg_features.10.bias torch.Size([256])\n",
            "encoder.vgg_features.12.weight torch.Size([256, 256, 3, 3])\n",
            "encoder.vgg_features.12.bias torch.Size([256])\n",
            "encoder.vgg_features.14.weight torch.Size([256, 256, 3, 3])\n",
            "encoder.vgg_features.14.bias torch.Size([256])\n",
            "encoder.vgg_features.16.weight torch.Size([256, 256, 3, 3])\n",
            "encoder.vgg_features.16.bias torch.Size([256])\n",
            "encoder.vgg_features.19.weight torch.Size([512, 256, 3, 3])\n",
            "encoder.vgg_features.19.bias torch.Size([512])\n",
            "encoder.vgg_features.21.weight torch.Size([512, 512, 3, 3])\n",
            "encoder.vgg_features.21.bias torch.Size([512])\n",
            "encoder.vgg_features.23.weight torch.Size([512, 512, 3, 3])\n",
            "encoder.vgg_features.23.bias torch.Size([512])\n",
            "encoder.vgg_features.25.weight torch.Size([512, 512, 3, 3])\n",
            "encoder.vgg_features.25.bias torch.Size([512])\n",
            "encoder.vgg_features.28.weight torch.Size([512, 512, 3, 3])\n",
            "encoder.vgg_features.28.bias torch.Size([512])\n",
            "encoder.vgg_features.30.weight torch.Size([512, 512, 3, 3])\n",
            "encoder.vgg_features.30.bias torch.Size([512])\n",
            "encoder.vgg_features.32.weight torch.Size([512, 512, 3, 3])\n",
            "encoder.vgg_features.32.bias torch.Size([512])\n",
            "encoder.vgg_features.34.weight torch.Size([512, 512, 3, 3])\n",
            "encoder.vgg_features.34.bias torch.Size([512])\n",
            "decoder.edge_feature.conv1.weight torch.Size([2048, 2048])\n",
            "decoder.edge_feature.conv1.bias torch.Size([2048])\n",
            "decoder.edge_feature.conv2.weight torch.Size([2048, 2048])\n",
            "decoder.edge_feature.conv2.bias torch.Size([2048])\n",
            "decoder.edge_feature.edge_linear.weight torch.Size([2048, 4096])\n",
            "decoder.edge_feature.edge_linear.bias torch.Size([2048])\n",
            "decoder.embedding.weight torch.Size([2994, 200])\n",
            "decoder.attention.encoder_linear.weight torch.Size([256, 4096])\n",
            "decoder.attention.encoder_linear.bias torch.Size([256])\n",
            "decoder.attention.decoder_linear.weight torch.Size([128, 256])\n",
            "decoder.attention.decoder_linear.bias torch.Size([128])\n",
            "decoder.attention.output_linear.weight torch.Size([256, 256])\n",
            "decoder.attention.output_linear.bias torch.Size([256])\n",
            "decoder.attention_self.linear.weight torch.Size([256, 200])\n",
            "decoder.attention_self.linear.bias torch.Size([256])\n",
            "decoder.attention_self.output_linear.weight torch.Size([200, 256])\n",
            "decoder.attention_self.output_linear.bias torch.Size([200])\n",
            "decoder.init_h.weight torch.Size([256, 4096])\n",
            "decoder.init_h.bias torch.Size([256])\n",
            "decoder.init_c.weight torch.Size([256, 4096])\n",
            "decoder.init_c.bias torch.Size([256])\n",
            "decoder.lstm_cell.weight_ih torch.Size([1024, 456])\n",
            "decoder.lstm_cell.weight_hh torch.Size([1024, 256])\n",
            "decoder.lstm_cell.bias_ih torch.Size([1024])\n",
            "decoder.lstm_cell.bias_hh torch.Size([1024])\n",
            "decoder.fcn.weight torch.Size([2994, 256])\n",
            "decoder.fcn.bias torch.Size([2994])\n"
          ]
        }
      ],
      "source": [
        "#init model\n",
        "learning_rate = 0.001\n",
        "model = EncoderDecoder(\n",
        "    embed_size=200,\n",
        "    vocab_size = len(train_dataset.vocab),\n",
        "    attention_dim=256,\n",
        "    gcn_encode_dim = 2048,\n",
        "    encoder_dim=4096,\n",
        "    decoder_dim=256\n",
        ").to(device)\n",
        "\n",
        "print(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "O-9XXHyos4kn"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "print_every = 100\n",
        "\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    for idx, (image, captions, graph) in enumerate(iter(data_loader)):\n",
        "        image,captions, graph= image.to(device),captions.to(device), graph.to(device)\n",
        "\n",
        "        # Zero the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed forward\n",
        "        outputs,attentions_cross, attentions_self = model(image, captions, graph)\n",
        "\n",
        "        # Calculate the batch loss.\n",
        "        targets = captions[:,1:]\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "\n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters in the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        if (idx+1)%print_every == 0:\n",
        "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
        "\n",
        "            #generate the caption\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                dataiter = iter(test_data_loader)\n",
        "                img,_,graph = next(dataiter)\n",
        "                features = model.encoder(img[0:1].to(device))\n",
        "                edge_features = model.decoder.edge_feature(graph.to(device))\n",
        "                encoded_input= torch.cat((features, edge_features), dim=2)\n",
        "                caps,alphas,alphas_self = model.decoder.generate_caption(encoded_input,vocab=train_dataset.vocab)\n",
        "                caption = ' '.join(caps)\n",
        "                show_image(img[0],title=caption)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbeQZltLs4ko"
      },
      "outputs": [],
      "source": [
        "#torch.save(model, 'model_gcn_attention.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK-9vr0ks4kp"
      },
      "outputs": [],
      "source": [
        "#helper function to save the model\n",
        "def save_model(model,num_epochs):\n",
        "    model_state = {\n",
        "        'num_epochs':num_epochs,\n",
        "        'embed_size':200,\n",
        "        'vocab_size':len(train_dataset.vocab),\n",
        "        'attention_dim':256,\n",
        "        'encoder_dim':2048,\n",
        "        'gcn_encoder_dim':4096,\n",
        "        'decoder_dim':256,\n",
        "        'state_dict':model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    torch.save(model_state,'gcn_attention_model_state_v2.pth')\n",
        "\n",
        "#save the latest model\n",
        "save_model(model,epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXRKC-5cs4kr"
      },
      "outputs": [],
      "source": [
        "# Load the checkpoint\n",
        "checkpoint = torch.load('gcn_attention_model_state_v4.pth',map_location=device)\n",
        "learning_rate = 0.001\n",
        "# Reinitialize the model and optimizer\n",
        "model = EncoderDecoder(\n",
        "    embed_size=200,\n",
        "    vocab_size = len(train_dataset.vocab),\n",
        "    attention_dim=256,\n",
        "    gcn_encode_dim = 2048,\n",
        "    encoder_dim=4096,\n",
        "    decoder_dim=256\n",
        ").to(device)  # Ensure this matches the original model's architecture\n",
        "\n",
        "# Create an instance of your model\n",
        "#expected_keys = model.state_dict().keys()\n",
        "\n",
        "# Remove keys that are not in expected_keys\n",
        "#filtered_state_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in expected_keys}\n",
        "\n",
        "# Assign the filtered state dictionary back to the checkpoint\n",
        "#checkpoint['state_dict'] = filtered_state_dict\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
        "# Load the filtered state dictionary into your model\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Load other saved data\n",
        "start_epoch = checkpoint['num_epochs'] + 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3wJOec6s4ks"
      },
      "outputs": [],
      "source": [
        "print_every = 200\n",
        "\n",
        "for epoch in range(start_epoch, 16):\n",
        "    for idx, (image, captions, graph) in enumerate(iter(data_loader)):\n",
        "        image,captions, graph= image.to(device),captions.to(device), graph.to(device)\n",
        "\n",
        "        # Zero the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed forward\n",
        "        outputs,attentions_cross, attentions_self = model(image, captions, graph)\n",
        "\n",
        "        # Calculate the batch loss.\n",
        "        targets = captions[:,1:]\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "\n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters in the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        if (idx+1)%print_every == 0:\n",
        "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
        "\n",
        "            #generate the caption\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                dataiter = iter(test_data_loader)\n",
        "                img,_,graph = next(dataiter)\n",
        "                features = model.encoder(img[0:1].to(device))\n",
        "                edge_features = model.decoder.edge_feature(graph.to(device))\n",
        "                encoded_input= torch.cat((features, edge_features), dim=2)\n",
        "                caps,alphas,alphas_self = model.decoder.generate_caption(encoded_input,vocab=train_dataset.vocab)\n",
        "                caption = ' '.join(caps)\n",
        "                show_image(img[0],title=caption)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hguxs1RUs4kt"
      },
      "outputs": [],
      "source": [
        "#helper function to save the model\n",
        "def save_model(model,num_epochs):\n",
        "    model_state = {\n",
        "        'num_epochs':num_epochs,\n",
        "        'embed_size':200,\n",
        "        'vocab_size':len(train_dataset.vocab),\n",
        "        'attention_dim':256,\n",
        "        'encoder_dim':2048,\n",
        "        'gcn_encoder_dim':4096,\n",
        "        'decoder_dim':256,\n",
        "        'state_dict':model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    torch.save(model_state,'gcn_attention_model_state_v4.pth')\n",
        "\n",
        "#save the latest model\n",
        "save_model(model,epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amIik8VNs4ku"
      },
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "rFS9fgWGs4kx"
      },
      "outputs": [],
      "source": [
        "#generate caption\n",
        "def get_caps_from(features_tensors,graph):\n",
        "    #generate the caption\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        features = model.encoder(features_tensors.to(device))\n",
        "        edge_features = model.decoder.edge_feature(graph.to(device))\n",
        "        encoded_input= torch.cat((features, edge_features), dim=2)\n",
        "        caps,alphas,alphas_self = model.decoder.generate_caption(encoded_input,vocab=train_dataset.vocab)\n",
        "        caption = ' '.join(caps)\n",
        "        show_image(features_tensors[0],title=caption)\n",
        "\n",
        "    return caps,alphas, alphas_self\n",
        "#Show attention\n",
        "def plot_attention(img, result, attention_plot):\n",
        "\n",
        "    #untransform\n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224\n",
        "    img[2] = img[2] * 0.225\n",
        "    img[0] += 0.485\n",
        "    img[1] += 0.456\n",
        "    img[2] += 0.406\n",
        "\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    temp_image = img\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        #attention_plot[l] = attention_plot[l][:,-1,:]\n",
        "        #print(attention_plot[l].shape)\n",
        "        temp_att_dim_1 = attention_plot[l][:,0,:].reshape(7,7)\n",
        "        temp_att_dim_2 = attention_plot[l][:,1,:].reshape(7,7)\n",
        "\n",
        "        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att_dim_1, cmap='gray', alpha=0.5, extent=img.get_extent())\n",
        "        ax.imshow(temp_att_dim_2, cmap='gray', alpha=0.7, extent=img.get_extent())\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydFSmmqas4kx"
      },
      "outputs": [],
      "source": [
        "for idx, (image, captions, graph) in enumerate(iter(test_data_loader)):\n",
        "  img = image[0].detach().clone()\n",
        "  img1 = image[0].detach().clone()\n",
        "  caps,alphas,alphas_self = get_caps_from(img.unsqueeze(0),graph)\n",
        "  plot_attention(img1, caps, alphas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "pJa4lP8ts4kz"
      },
      "outputs": [],
      "source": [
        "#show any 1\n",
        "dataiter = iter(test_data_loader)\n",
        "images,_,graph = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas,alphas_self = get_caps_from(img.unsqueeze(0),graph)\n",
        "plot_attention(img1, caps, alphas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6SKbx8fs4k6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}